{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shmuel Ruppo - \n",
    "## Analyzing Global Terrorist Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>In the GTD database, the groups who carried out some of the terror attacks have not \n",
    "been identified. The task is to build a model that predicts the responsible group of \n",
    "those unidentified attacks</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import h2o\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Importing a large file in CSV format is 10-20 times faster than importing an Excel file.\n",
    "Using MS Excel, I have converted the Excel file provided to CSV. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "df = pd.read_csv('all_dataset.csv', header=0, low_memory=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Throughout all the columns, the values -99, 9 stand for the 'Unknown' category.\n",
    "Therefore, they all can be replaced with nan </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace=[-99, -9], value=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "Using the codebook provided by GTD (https://www.start.umd.edu/gtd/downloads/Codebook.pdf),\n",
    "and double-checking through the excel file, I have learned that every textual categorical\n",
    "variable in the dataset is already encoded as a numerical categorical variable.\n",
    "For example, the list of countries : 'country_txt' is encoded as a numeric list of countries\n",
    "'country'. Thus, I can work only with numeric variables. \n",
    "But, an important exception is the name of the terror group 'gname', which is not encoded.\n",
    "So, I leave it as is. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df._get_numeric_data().join(df['gname'])\n",
    "\n",
    "#Also, 'eventid' uniquely identifies each terror act. But it does not carry any meaning within it,\n",
    "#and therefore can be dropped\n",
    "\n",
    "df = df.drop('eventid', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>The name of the terror group is the response variable.\n",
    "Terror groups that have been identified a small amount of times will likely  add more\n",
    "noise than information. Thus, I remove terror groups that have carried out less (or equal to)\n",
    "than 5 terror acts. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177093, 77)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = ( pd.value_counts(df.loc[:,'gname']))\n",
    "df = df.loc[df['gname'].isin(temp.index[temp > 5]),:]\n",
    "\n",
    "\n",
    "# Display the dimensions of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>I distinguish between numeric and categorical variables, making a list of each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of the numeric variables:\n",
    "numericList = [\n",
    " 'nkill', 'nkillus', 'nkillter', 'nwound', 'nwoundus', \n",
    "    'nwoundte', 'nhostkid', 'iyear', 'imonth', 'iday','nhostkidus',  'ransomamt', 'ransomamtus',\n",
    "'ransompaid', 'ransompaidus', 'nreleased', 'propvalue', 'latitude', 'longitude']\n",
    "\n",
    "\n",
    "# Get the location of numeric\n",
    "locNumeric = [df.columns.get_loc(c) for c in numericList]\n",
    "\n",
    "#Whatever is not numeric, is categoric\n",
    "categNum = [i for i in range(len(df.columns)) if i not in locNumeric]\n",
    "categList = [i for i in df.columns if i not in numericList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>I separate, and put  aside the terror acts that have not been identified as belonging to a \n",
    "specific terror group. I will use them for the final prediction</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_index = df.loc[(df['gname'] == 'Unknown'), 'gname'].index\n",
    "\n",
    "prd_group = df.loc[mask_index,:]   # Will be used for the final prediction\n",
    "df = df.drop(mask_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model : Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>I will use the Random Forest method for classification of the data.\n",
    "The Random Forest method is possible to use with a large amount of features.\n",
    "It is highly economical in terms of preprocessing. It handles both categorical \n",
    "and numerical variables. (The numerical data does not need scaling). \n",
    "\n",
    "Rather than guessing \\ deciding using an expert opinion (which is unavailable to me)\n",
    "what are the most important features, it is possible to analyze the features that contribute\n",
    "the most to the Random Forest technique.\n",
    "\n",
    "My strategy is:\n",
    "First, use a Random Forest model on a small subset of the database.\n",
    "See what are the features that contribute most to the random forest.\n",
    "Then use those features on a larger subset of the database.\n",
    "\n",
    "I will use the H2O library. \n",
    "A benefit of using H2O library is its efficient use of categorical variables.\n",
    "It deals with categorical variables without needing one-hot encoding. </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) Client VM (build 25.191-b12, mixed mode)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\backend\\server.py:369: UserWarning:   You have a 32-bit version of Java. H2O works best with 64-bit Java.\n",
      "  Please download the latest 64-bit Java SE JDK from Oracle.\n",
      "\n",
      "  warn(\"  You have a 32-bit version of Java. H2O works best with 64-bit Java.\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Starting server from C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: c:\\users\\user\\appdata\\local\\temp\\tmp8xwacc\n",
      "  JVM stdout: c:\\users\\user\\appdata\\local\\temp\\tmp8xwacc\\h2o_user_started_from_python.out\n",
      "  JVM stderr: c:\\users\\user\\appdata\\local\\temp\\tmp8xwacc\\h2o_user_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54323\n",
      "Connecting to H2O server at http://127.0.0.1:54323... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Asia/Jerusalem</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>11 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_user_gpt3g2</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>989 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54323</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.15 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       Asia/Jerusalem\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.2\n",
       "H2O cluster version age:    11 days\n",
       "H2O cluster name:           H2O_from_python_user_gpt3g2\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    989 Mb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54323\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.15 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:1472: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "seconds it took 51.649469203\n",
      "Top-10 Hit Ratios: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>k</b></td>\n",
       "<td><b>hit_ratio</b></td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>0.7895002</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.8399798</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.861686</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.8768299</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.8828874</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.8864210</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.8944977</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.8985361</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.9010600</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.9030792</td></tr></table></div>"
      ],
      "text/plain": [
       "k    hit_ratio\n",
       "---  -----------\n",
       "1    0.7895\n",
       "2    0.83998\n",
       "3    0.861686\n",
       "4    0.87683\n",
       "5    0.882887\n",
       "6    0.886421\n",
       "7    0.894498\n",
       "8    0.898536\n",
       "9    0.90106\n",
       "10   0.903079"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "t=time.clock()\n",
    "\n",
    "# Differentiate between training and response variables\n",
    "training_columns=list(df.columns.difference(['gname']))\n",
    "response_column = ['gname']\n",
    "\n",
    "# Create sample the data\n",
    "random.seed(123)\n",
    "mysample = random.sample(range(df.shape[0]), 10000)\n",
    "\n",
    "# init H2O\n",
    "h2o.init(max_mem_size = '1g')\n",
    "h2o.remove_all()\n",
    "\n",
    "# convert the data into H2OFrame\n",
    "hf = h2o.H2OFrame(df.loc[mysample,:])\n",
    "\n",
    "# Distinguish between categoric and numeric features\n",
    "hf[:,numericList]=hf[:,numericList].asnumeric() \n",
    "hf[:,categList] = hf[:,categList].asfactor()\n",
    "\n",
    "## Split the data into a train and a test set.  Random Forest will be tested on the latter.\n",
    "train, test = hf.split_frame(ratios=[0.8], seed = 123)\n",
    "\n",
    "\n",
    "# Do not select a large number of trees grown, and too large a depth initially\n",
    "model = H2ORandomForestEstimator(  max_depth=5, ntrees = 5, seed= 123)\n",
    "\n",
    "# Train model\n",
    "\n",
    "model.train(x=training_columns, y=str(response_column[0]), training_frame=train)\n",
    "\n",
    "# Test the model\n",
    "performance = model.model_performance(test_data=test)\n",
    "print 'seconds it took', time.clock() - t\n",
    "print performance.hit_ratio_table()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>I am taking only the most important features:\n",
    "I have selected the features whose percentage of contribution is more than 1%</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        variable  percentage\n",
      "0        country    0.129785\n",
      "1        natlty1    0.123128\n",
      "2           iday    0.106729\n",
      "3    specificity    0.099089\n",
      "4        success    0.097614\n",
      "5       extended    0.097484\n",
      "6     individual    0.096568\n",
      "7   targsubtype1    0.060893\n",
      "8       latitude    0.030478\n",
      "9          iyear    0.012483\n",
      "10  weapsubtype1    0.011800\n"
     ]
    }
   ],
   "source": [
    "importance = model.varimp(use_pandas=True)\n",
    "\n",
    "importance = importance[importance['percentage']>0.01]\n",
    "\n",
    "print importance[['variable', 'percentage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>This is what the feature names stand for :\n",
    "\n",
    "country       The country where the incident occured\n",
    "natlty1       The nationality of the target that was attacked\n",
    "iday          Day of the month\n",
    "specificity   The precision with which the latitude and the longitude are known\n",
    "success       Whether there was a tangible effect of the attack \n",
    "extended      Whether the duration of the attack was more than 24 hours\n",
    "targsubtype1  The specific category to which the target belongs\n",
    "latitude      Latitude of the attack\n",
    "iyear         Year of the attack\n",
    "weapsubtype1  Specific category of the weapon used \n",
    "\n",
    "The selected features \"make sense\". For example, it is expected that the country the attack\n",
    "takes place in would be of high importance in the explanation.\n",
    "\n",
    "The next step is fine-tuning the model with the important features only. First, the set-up:</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "#Set-up the model with the important features\n",
    "\n",
    "training_columns=list(importance['variable'])\n",
    "response_column = ['gname']\n",
    "\n",
    "df2 = df[training_columns+ response_column]\n",
    "\n",
    "random.seed(123)\n",
    "mysample = random.sample(range(df2.shape[0]), 10000)\n",
    "\n",
    "df2 = df2.loc[mysample,:]\n",
    "\n",
    "hf2 = h2o.H2OFrame(df2)\n",
    "\n",
    "# For the sake of distinguishing between categorical and numeric variables\n",
    "# I distinguish between numerical variables selected as important:\n",
    "numericList_imp = list(np.intersect1d(numericList, importance['variable']))\n",
    "\n",
    "\n",
    "# and the  categoric variables selected as important\n",
    "categList_imp = list(np.intersect1d(categList, importance['variable']))\n",
    "#The name of the terror group is also categorical\n",
    "categList_imp.append('gname')\n",
    "\n",
    "# Make the distinction between numeric and categoric variables\n",
    "hf2[:,numericList_imp]=hf2[:,numericList_imp].asnumeric() \n",
    "hf2[:,categList_imp] = hf2[:,categList_imp].asfactor()\n",
    "\n",
    "\n",
    "# Again split into train and test\n",
    "\n",
    "train, test = hf2.split_frame(ratios=[0.8], seed = 123)\n",
    "\n",
    "\n",
    "# A function to iterate over the different parameters of the Random Forest model\n",
    "\n",
    "def EstimateModel( ntrees,  max_depth ):\n",
    "    t=time.clock()\n",
    "    global model2\n",
    "    \n",
    "    # Initialize the model\n",
    "    model2 = H2ORandomForestEstimator( ntrees = ntrees, max_depth = max_depth, seed= 123)\n",
    "    # Train the model\n",
    "    model2.train(x=training_columns, y=str(response_column[0]), training_frame=train)\n",
    "    # Predict performance\n",
    "    performance = model2.model_performance(test_data=test)    \n",
    "    \n",
    "    #Display parameters, performance, and the amount of time the analysis took\n",
    "    print 'ntrees = ', ntrees, 'max_depth=', max_depth, 'Hit ratio=', performance.hit_ratio_table()[1][0]\n",
    "    print 'time it took', time.clock() - t, 'seconds'\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>The next step I took is iterating over a combination of the parameters to find the best.\n",
    "I have created a vector of 5 different values for the amount of trees to be grown, \n",
    "and a vector of 5 values for the depth parameter. \n",
    "\n",
    "However, I do not check each of the 25 possible combinations.\n",
    "Rather, the initial step is to increment both parameters each iteration.\n",
    "\n",
    "I will use the hit-ratio as the metric to measure success.  It is the number of \n",
    "times that a correct prediciton has been made divided by  the total number of predictions. \n",
    "It is an easy-to-communicate parameter</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  8 max_depth= 3 Hit ratio= 0.79202425\n",
      "time it took 21.2996396039 seconds\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  16 max_depth= 5 Hit ratio= 0.81120646\n",
      "time it took 34.4845220961 seconds\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  20 max_depth= 7 Hit ratio= 0.81272084\n",
      "time it took 49.2989677976 seconds\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  24 max_depth= 9 Hit ratio= 0.8192832\n",
      "time it took 58.2209678209 seconds\n",
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  28 max_depth= 10 Hit ratio= 0.819788\n",
      "time it took 69.7933047971 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a vector\n",
    "ntrees_list = (8,16,20, 24, 28) \n",
    "max_depth_list = (3,5,7,9,10)\n",
    "\n",
    "\n",
    "for i in range(len(ntrees_list)):\n",
    "    EstimateModel(ntrees_list[i], max_depth_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>The changes in the hit ratio become rather small.\n",
    "\n",
    "When I decrease the number of trees, it has an additional small positive effect:</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  24 max_depth= 10 Hit ratio= 0.8202928\n",
      "time it took 61.5633231085 seconds\n"
     ]
    }
   ],
   "source": [
    "EstimateModel(ntrees=24, max_depth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### The hit-ratio on the validation set  is 82% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Not all of the terror acts in the GTD database have been identified with a certain group.\n",
    "The task of the challenge is to predict the groups responsible for those un-identified terror\n",
    "acts. Though of course it's impossible to verify the truth of their predictions using the\n",
    "current database, a hit-ratio of 82% on the validation set does look good (especially if we \n",
    "take into account the large number of terror groups that are involved.) </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>I use the parameters that gave the highest hit-ratio so far, and\n",
    "make the final prediction</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "ntrees =  24 max_depth= 10 Hit ratio= 0.8202928\n",
      "time it took 73.5661451615 seconds\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\job.py:69: UserWarning: Test/Validation dataset column 'country' has levels not trained on: [5, 10, 12, 17, 18, 22, 23, 25, 31, 32, 33, 35, 37, 38, 46, 47, 51, 54, 57, 62, 63, 64, 67, 68, 70, 72, 73, 76, 80, 85, 89, 90, 91, 99, 100, 103, 107, 108, 109, 113, 115, 116, 117, 119, 120, 121, 122, 124, 127, 128, 129, 132, 143, 144, 161, 166, 175, 178, 179, 180, 181, 184, 189, 197, 201, 203, 204, 207, 215, 219, 223, 226, 347, 349, 377, 428, 1001, 1002, 1004]\n",
      "  warnings.warn(w)\n",
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\job.py:69: UserWarning: Test/Validation dataset column 'natlty1' has levels not trained on: [10, 12, 18, 22, 23, 24, 25, 28, 29, 31, 32, 33, 35, 37, 42, 46, 49, 50, 54, 55, 57, 62, 63, 64, 67, 68, 70, 72, 73, 76, 80, 81, 85, 89, 90, 91, 99, 100, 103, 107, 108, 109, 115, 116, 117, 119, 120, 121, 122, 124, 125, 127, 128, 129, 132, 134, 143, 144, 146, 149, 152, 161, 164, 166, 176, 178, 179, 180, 181, 189, 197, 201, 202, 203, 204, 207, 210, 215, 219, 221, 223, 225, 236, 347, 377, 603, 604, 1001, 1002, 1004]\n",
      "  warnings.warn(w)\n",
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\job.py:69: UserWarning: Test/Validation dataset column 'targsubtype1' has levels not trained on: [33, 59, 63, 64, 91]\n",
      "  warnings.warn(w)\n",
      "C:\\Users\\user\\Anaconda2\\lib\\site-packages\\h2o\\job.py:69: UserWarning: Test/Validation dataset column 'weapsubtype1' has levels not trained on: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31]\n",
      "  warnings.warn(w)\n"
     ]
    }
   ],
   "source": [
    "EstimateModel(ntrees=24, max_depth = 10)\n",
    "\n",
    "#Preparing the final test-set\n",
    "prd_group = prd_group.drop('gname', axis=1)\n",
    "hf_pred = h2o.H2OFrame(prd_group)\n",
    "\n",
    "# Differentiate between numeric and categoric variables \n",
    "hf_pred[:,numericList_imp]=hf_pred[:,numericList_imp].asnumeric() \n",
    "hf_pred[:,categList_imp[:-1]] = hf_pred[:,categList_imp[:-1]].asfactor()\n",
    "\n",
    "# Make the prediction\n",
    "final_pred = model2.predict(hf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Though using the current dataset only, it is not possible to verify the final results,\n",
    "a basic feasability-check can be performed.\n",
    "I show all the groups identified as being responsible for terror acts in Israel (my country),\n",
    "grouped by the number of identifications made for each group.</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Palestinians                                               463\n",
       "Palestinian Islamic Jihad (PIJ)                             94\n",
       "Hamas (Islamic Resistance Movement)                         91\n",
       "Democratic Front for the Liberation of Palestine (DFLP)     82\n",
       "Al-Aqsa Martyrs Brigade                                     36\n",
       "Popular Front for the Liberation of Palestine (PFLP)         6\n",
       "Al-Qaida in Iraq                                             1\n",
       "Mahdi Army                                                   1\n",
       "Real Irish Republican Army (RIRA)                            1\n",
       "Thai Islamic Militants                                       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = hf_pred.cbind(final_pred['predict']).as_data_frame()\n",
    "results[results['country'] == 97].loc[:,['predict']].stack().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Those group names are familiar to Israeli citizens as groups that performed terror acts\n",
    "in Israel in the past (even though the group 'Palestinians' is too vague to be useful). \n",
    "Some groups seem to be definitely mistaken, but the number of their occurence is very small. \n",
    "All in all, the results do appear sensible.\n",
    "\n",
    "An area of future improvement is that it is desired to use a smarter algorithm for finding the best parameters of the model. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure you want to shutdown the H2O instance running at http://127.0.0.1:54323 (Y/N)? Y\n",
      "H2O session _sid_ba9c closed.\n"
     ]
    }
   ],
   "source": [
    "#Shutdown H2O\n",
    "h2o.cluster().shutdown(prompt = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
